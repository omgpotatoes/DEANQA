
%\documentclass{acm_proc_article-sp}
\documentclass[11pt,letterpaper]{article}

%==================================
% packages go here
%==================================

\usepackage{url}
\usepackage{graphicx}
\usepackage{cite} % sort citation numbers
\usepackage{color}
\usepackage{soul}
\usepackage{naaclhlt2012}
\usepackage{times}
\usepackage{latexsym}

%\usepackage{ntheorem}
%\usepackage{amsthm}
%\usepackage{multirow} % for complex tables
%\usepackage{times}
%\usepackage[pdftex]{hyperref}
%\hypersetup{colorlinks=false, pdfborder= 0 0 0}
%\usepackage[draft,inline,nomargin]{fixme}
%\newcommand{\hlfixme}[1]{\fixme{\hl{#1}}}
%\newcommand{\hlfxnote}[1]{\fxnote{\hl{#1}}}

%\newcommand{\name}{\emph{Watson Jr.}}
\newcommand{\name}{\emph{DeanQA}}

\setlength\titlebox{6.5cm}    % Expanding the titlebox

%==================================

%\begin{document}

\title{\name: \\An Advanced Question Answering System}

\author{
	Yann Le Gall\\
    University of Pittsburgh\\
	Department of Computer Science\\
    {\tt ylegall@cs.pitt.edu}
\And
	Author 2\\
  	ABC University\\
  	900 Main Street\\
  	Ourcity, PQ, Canada A1A 1T2\\
	{\tt author2@abc.ca}
\And
	Author 3\\
    XYZ Company\\
    111 Anywhere Street\\
    Mytown, NY 10000, USA\\
    {\tt author1@xyz.org}
}

\date{12 December 2011}

\begin{document}

\maketitle

%==================================

\begin{abstract}
abstract goes here
\end{abstract}

% A category with the (minimum) three required fields
%\category{I.2}{Artifical Intelligence}{Natural Language Processing}
%\terms{Algorithms, Experimentation}
%\keywords{Question Answering}

%==================================

\section{Introduction}
\label{sec:intro}

In this paper, we design, implement, and evaluate a question answering
(QA) system, \name. In our approach, we use several different strategies from
different domains to select potential answers. Then, we employ a
majority voting scheme to combine the results.

% TODO: should this description of the dataset go in another section?
To train and test our QA system, we used the ``CBC Reading
Comprehension Corpus''. This corpus is composed of 125 news stories,
each accompanied by a set of 6-10 factoid questions (e.g. questions
that begin with ``Who'', ``When'', ``Where'', etc.).
%The news stories were obtained from the ``CBC 4 Kids'' website,
%hosted by the Canadian Broadcast Corporation. The questions and an
%answer key were added by the MITRE Corporation, and are in the style
%of actual reading comprehension tests that are given to grade school
%children in the United States.

All stories in the dataset were been split into sentences (one
sentence per line) using the MXTERMINATOR sentence splitter developed
by Adwait Ratnaparkhi.  Paragraphs from the original story are
separated by an empty line. 

The rest of this paper is organized as follows: in
\S\ref{sec:implementation} we describe the design and implementation
of our QA system and each of its sub-components. Next, in
\S\ref{sec:evaluation} we evaluate our system on the test dataset and
present the performance results. Finally, we conclude in
\S\ref{sec:conclusion}.


\section{Implementation}
\label{sec:implementation}

In this section we discuss the design and implementation of \name
. First, we explain the general, overall structure. Then we give
a detailed description of the sub-components (\emph{AnswerFinders}) of
the system. Finally, we present our technique for combining the
potential answers from each sub-component.

\begin{figure*} 
	\centering
	\includegraphics[width=0.8\textwidth]{model.png}
	\caption{The \name system model.}
	\label{fig:model}
\end{figure*}

Figure \ref{fig:model} shows the general design of \name.
The system takes processes a document and a set of questions as input.
At this point, several preprocessing steps might be performed,
including the removal of stop words, coreference resolution, etc.
Next, the processed document and the current question are passed to
each of the AnswerFinders. Each of these AnswerFinders implements a
different strategy for finding answers to the question, and they
each return a collection of \texttt{<answer,confidence>} pairs.
Finally, the potential answers are combineded in a voting algorithm
that gives different weights to the potential answers based on the
confidence and the question type.

In the next few subsections, we give more detailed descriptions of
the implementation of the different AnswerFinders and the
strategies that they use.

\subsection{Bag-of-Words}

The Bag-of-words (BOW) model is a simple model that provides relatively good
performance in our systeml. In our domain, BOW treats each question as a set of
words. Then, for each sentence in the document, we count the number of matching
words. The sentence with the largest number of matching words is chosen as the
answer.

\subsection{Linguistic Rule-based Strategies}

We also implemented a rule-based AnswerFinder based on the previous
work of \cite{riloff2000}. This system applies different rules based
on the question type. A preliminary score is calculated for each line
based on the BOW model. Then, different amounts of points are applied
for each rule that is satisfied.

\subsection{NLP Components}

% TODO: Alex writes this

\subsection{SVM Classification}

% TODO: Eric writes this part

\subsection{Combining Answers with Majority Voting}
Previous work by Rotaru and Litman demonstrates that combining
the outputs of multiple QA systems can achieve better results than the
individual systems alone \cite{rotaru2005}. We incorporate this idea
into the design of our QA system by combining the outputs from each of
the subsystems described above.

% TODO: describe how we combine the different results

\section{Evaluation}
\label{sec:evaluation}

%******************************************************************
%Accuracy: 181 correct out of 310 questions - 58.39%.
%******************************************************************
%NORMALIZED ACCURACY : 60.89%

\begin{table}
\centering

	\begin{tabular}{|r|c|c|}
	\hline
	Type   & \# Correct & Percentage \\
	\hline
	\hline
	WHEN   &  25 out of  34 &  73.53\% \\
	\hline
	WHERE  &  23 out of  36 &  63.89\% \\
	\hline
	WHAT   &  42 out of  80 &  52.50\% \\
	\hline
	WHY    &  26 out of  44 &  59.09\% \\
	\hline
	WHO    &  26 out of  41 &  63.41\% \\
	\hline
	HOW    &  36 out of  68 &  52.94\% \\
	\hline
	OTHER  &   3 out of   7 &  42.86\% \\
	\hline
	\hline
	TOTAL  & 181 out of 310 &  58.39\% \\
	\hline
	\end{tabular}

\caption{Statistics by question type.}
\label{table:question-types}
\end{table}

%
%WHEN   :   25 out of   34 -  73.53\%
%WHERE  :   23 out of   36 -  63.89\%
%WHAT   :   42 out of   80 -  52.50\%
%WHY    :   26 out of   44 -  59.09\%
%WHO    :   26 out of   41 -  63.41\%
%HOW    :   36 out of   68 -  52.94\%
%OTHER  :    3 out of    7 -  42.86\%
%TOTAL  :  181 out of  310 -  58.39\%
%TOTAL  & 181 out of 310 &  58.39\% \\
%




\section{Conclusions}
\label{sec:conclusion}

Question-answering is complex task and a cutting-edge research area
in natural language processing. 


%ACKNOWLEDGMENTS are optional
%\section{Acknowledgments}

%\bibliographystyle{abbrv}
%\bibliographystyle{plain}
\bibliographystyle{naaclhlt2012}
%\bibliographystyle{acl}
\bibliography{references} 

%
%\appendix
%%Appendix A
%\section{Headings in Appendices}
%\balancecolumns


\end{document}

